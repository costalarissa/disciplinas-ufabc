{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343744a0",
   "metadata": {},
   "source": [
    "# Sistema de Detec√ß√£o de Equival√™ncia de Disciplinas\n",
    "\n",
    "# Notebook de verifica√ß√£o da similaridade entre ementas de disciplinas\n",
    "\n",
    "## Introdu√ß√£o:\n",
    "O estudante da UFABC passa muito tempo j√° em sua gradua√ß√£o para terminar as disciplinas do seu BI e do p√≥s-BI e isso afeta principalmente os alunos de cursos mais concorridos como √© o de computa√ß√£o, em que as disciplinas podem ter mais de 150% de requisi√ß√£o. Tendo em vista isso, na UFABC temos dois processos j√° estruturados que √© o processo de covalida√ß√£o e de equival√™ncia, normatizados nas resolu√ß√µes ConsEPE n¬∫ 157/2013 e CG N¬∫ 023/2019 respectivamente. Covalida√ß√£o √© um processo interno da UFABC que √© basicamente para a transi√ß√£o de projetos pedag√≥gicos, de forma que o(a) estudante consegue integralizar o curso em um PPC antigo com disciplinas novas e a equival√™ncia √© um processo que uma disciplina de fora pode ter alguma similaridade de uma disciplina da ufabc e do curso que voc√™ quer se formar. Assim est√° presente na Resolu√ß√£o CG N¬∫ 023/2019 o seguinte:\n",
    "\n",
    ">Art. 4¬∫ Consistem em requisitos para a dispensa por equival√™ncia, para disciplinas\n",
    "cursadas no Brasil:\n",
    "\n",
    ">> I. a carga hor√°ria total da disciplina cursada deve ser igual ou maior √† carga hor√°ria da que se pede equival√™ncia;\n",
    "\n",
    ">> II. o conte√∫do da disciplina cursada deve ser compat√≠vel e correspondente a, no m√≠nimo, 75% (setenta e cinco por cento) do conte√∫do daquela de que se pede equival√™ncia, considerando-se teoria e pr√°tica, quando for o caso. \n",
    "\n",
    ">>>Par√°grafo √∫nico: Excepcionalmente, e mediante justificativa, a coordena√ß√£o de curso pode autorizar equival√™ncias que cumpram parcialmente estes requisito\n",
    "\n",
    "Utilizando essa base das normativas e os cat√°logos de disciplinas da UFABC, objetivamos gerar um sistema de pr√©-avalia√ß√£o de disciplinas com alta chance de convalida√ß√£o, reduzindo o espa√ßo de busca dos t√©cnicos responsaveis pela aprova√ß√£o de pares de disciplinas com convalida√ß√£o e garantindo a inclus√£o de todas as disciplinas na an√°lise. Os benef√≠cio desta proposta s√£o a economia de recursos, maior integraliza√ß√£o de diferentes PPCs e promove efetivamente a interdisciplinaridade, fundamento da UFAB, uma vez que os cursos poderiam ofertar a mesma disciplina com diferentes enfoques no mesmo quadrimestre, melhorando a qualidade do ensino. Os recursos poupados s√£o tanto na carga de trabalho dos t√©cnicos quanto recursos computacionais (realizada manualmente, ess tarefa tem um complexidade O(n^2)), requerindo an√°lise manual apenas das disciplinas que se encaixam nos crit√©rios das resolu√ß√µes ConsEPE n¬∫ 157/2013 e CG N¬∫ 023/2019 e com alta probabilidade de valida√ß√£o. \n",
    "\n",
    "Dessa forma propomos uma an√°lise de similaridade semantica e relacional entre as ementas, os pr√©-requisitos das disciplinas da UFABC e dos valores de TPEI, para fim de pr√©-avalia√ß√£o de equival√™ncias entre as diciplinas da universidade cumprindo com o Art. 4¬∞ da resolu√ß√£o CG N¬∫ 023/2019. Assim. em disciplinas que verificarmos que existe uma similaridade equivalente a de pares de disciplinas atualmente validadas (similaridade maior ou igual a 75% e que cumpre a quantidade de creditos da outra disciplina), poderemos gerar listas de pares para aprova√ß√£o manual em ordem de similaridde. O benef√≠cio proporcionado pela maior integra√ß√£o do sistema √© dif√≠cil de mensurar mas, al√©m de afetar todes es envolvides no processo discente, o alinhamento dessa solu√ß√£o com os sistemas vigentes da universidade amplificam seu impacto e condi√ß√µes em que ele se manifesta.\n",
    "\n",
    "## Objetivos\n",
    "1. Fazer uma proposta de equival√™ncia interna de disciplinas a partir da an√°lise de similaridade sem√¢ntica entre as ementas das disciplinas da UFABC da Gradua√ß√£o e da quantidade de TPEI que elas tem\n",
    "2. Reduzir o espa√ßo de busca de O(n¬≤) para aproximadamente O(n*log(n)) a partir da aplica√ß√£o de filtros de TPEI para quantidade de cr√©ditos das disciplinas, de forma s√≥ comparar as diciplinas que tem a quantidade de cr√©ditos iguais\n",
    "\n",
    "## M√©todos\n",
    "1. Embedding sem√¢ntico do conte√∫do dos Objetivos e Ementas das disciplinas utilizando BERTimbau\n",
    "2. C√°lculo de similaridade cosseno para similaridade entre os textos \n",
    "3. Kmeans nos embeddings para determinar agrupamentos de disciplinas ajudando no processo de equival√™ncia\n",
    "4. Grafo relacional dos Pr√©-Requisitos entre disciplinas\n",
    "5. C√°lculo de similaridade utiliando dist√¢ncias de jaccard\n",
    "6. Otimiza√ß√£o dos hiperpar√¢metros dos filtros de redu√ß√£o do espa√ßo de busca utilizando modelos de √°rvores aleat√≥rias \n",
    "\n",
    "## 1. Imports e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3772ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\angel\\Documents\\base_dados_disciplinas_ufabc\\.venv_disciplinas\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importando os pacotes necess√°rios\n",
    "'''pip install pandas nltk scikit-learn ipykernel \"transformers<4.26.0\" sentence-transformers openpyxl matplotlib networkx seaborn plotly node2vec'''\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import unicodedata\n",
    "import requests\n",
    "from io import StringIO\n",
    "from node2vec import Node2Vec\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85da40",
   "metadata": {},
   "source": [
    "## 2. Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99f4b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def carregar_catalogo():\n",
    "    '''Usa o cat√°logo da UFABC direto do Reposit√≥rio no GitHub e retorna um DataFrame com os dados.'''\n",
    "    \n",
    "    print(\"üîÑ Baixando cat√°logo de disciplinas...\")\n",
    "    df = pd.read_excel('catalogo_disciplinas_graduacao_2024_2025.xlsx', engine='openpyxl')\n",
    "\n",
    "    print(\"‚úÖ Download bem-sucedido!\")\n",
    "    print(\"üìù Colunas dispon√≠veis:\", df.columns.tolist())\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7a644",
   "metadata": {},
   "source": [
    "## 3. Preprocessamento e prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    '''Normaliza o texto removendo acentua√ß√£o, pontua√ß√£o e stopwords. Al√©m disso, aplica stemming, reduzindo a palavra ao seu radical.'''\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    norm = unicodedata.normalize('NFKD', str(text))\n",
    "    norm = norm.encode('ASCII', 'ignore').decode('utf-8')\n",
    "    norm = re.sub(r'[^\\w\\s]', '', str(text).lower())\n",
    "    norm = norm.lower().strip()\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def extract_tpei(tpei_str):\n",
    "    '''Extrai os valores de TPEI (Teoria, Pr√°tica, Extens√£o e Individual) de uma string no formato \"T-P-E-I\" e retorna um dicion√°rio com os valores.\n",
    "        Se a string for inv√°lida, retorna um dicion√°rio vazio.'''\n",
    "    if pd.isna(tpei_str):\n",
    "        return []\n",
    "    \n",
    "    values = tpei_str.split('-')\n",
    "    # print(values)\n",
    "    return {\n",
    "        'teoria': int(values[0]),\n",
    "        'pratica': int(values[1]),\n",
    "        'extensao': int(values[2]),\n",
    "        'individual': int(values[3]),\n",
    "        'total_creditos': int(values[0]) + int(values[1])  # T+P only\n",
    "    }\n",
    "\n",
    "def extract_prereq(recomendacao):\n",
    "    '''Extrai os pr√©-requisitos de uma string no formato \"Disciplina1; Disciplina2; ...\" e retorna uma lista com os c√≥digos das disciplinas.\n",
    "        Se a string for inv√°lida, retorna uma lista vazia.'''\n",
    "    if pd.isna(recomendacao) or recomendacao.strip() == '' or recomendacao == 'N√£o h√°':\n",
    "        return []\n",
    "\n",
    "    prereqs = []\n",
    "    for part in recomendacao.split(';'):\n",
    "        part = part.strip()\n",
    "        if part:\n",
    "            prereqs.append(part)\n",
    "    return prereqs\n",
    "\n",
    "def extract_cod(sigla):\n",
    "    '''Extrai o c√≥digo da disciplina de uma string no formato \"SIGLA\" e retorna uma lista com o c√≥digo.\n",
    "        Se a string for inv√°lida, retorna uma lista vazia.'''\n",
    "    if pd.isna(sigla):\n",
    "        return []\n",
    "    return normalize_text(sigla)\n",
    "\n",
    "def create_allfeats(df):\n",
    "    '''Cria todas as features necess√°rias para o modelo de recomenda√ß√£o. Retorna um DataFrame com as features criadas.'''\n",
    "    df = df.copy()\n",
    "\n",
    "    # TPEI\n",
    "    tpei_feats = df['TPEI'].apply(extract_tpei)\n",
    "    df['teoria'] = tpei_feats.apply(lambda x: x['teoria'])\n",
    "    df['pratica'] = tpei_feats.apply(lambda x: x['pratica'])\n",
    "    df['extensao'] = tpei_feats.apply(lambda x: x['extensao'])\n",
    "    df['individual'] = tpei_feats.apply(lambda x: x['individual'])\n",
    "    df['total_creditos'] = tpei_feats.apply(lambda x: x['total_creditos'])\n",
    "\n",
    "    # Ementa\n",
    "    df['ementa_norm'] = df['EMENTA'].apply(normalize_text)\n",
    "    df['objetivos_norm'] = df['OBJETIVOS'].apply(normalize_text)\n",
    "\n",
    "    # Pr√©-requisitos\n",
    "    df['prerequisites'] = df['RECOMENDA√á√ÉO'].apply(extract_prereq)\n",
    "    df['num_prerequisites'] = df['prerequisites'].apply(len)\n",
    "    df['codigo'] = df['SIGLA'].apply(extract_cod)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Baixando cat√°logo de disciplinas...\n",
      "‚úÖ Download bem-sucedido!\n",
      "üìù Colunas dispon√≠veis: ['SIGLA', 'DISCIPLINA', 'TPEI', 'RECOMENDA√á√ÉO', 'OBJETIVOS', 'METODOLOGIA EXTENSIONISTA', 'EMENTA', 'BIBLIOGRAFIA B√ÅSICA', 'BIBLIOGRAFIA COMPLEMENTAR']\n"
     ]
    }
   ],
   "source": [
    "#### inicializar os dados\n",
    "#carregar o cat√°logo de disciplinas\n",
    "df = carregar_catalogo()\n",
    "# realizar o pr√©-processamento dos dados criando as features\n",
    "df = create_allfeats(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd40eb",
   "metadata": {},
   "source": [
    "## 4. Filtro TPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pair in pairs\n",
    "#   se total creditos[current] > total_creditos[comp]\n",
    "#       remove.pair\n",
    "#   else\n",
    "#       tpei_dif = comp - current\n",
    "#\n",
    "# Se os cr√©ditos totais da disciplina sendo comparada for maior do que da disciplina sendo analisada\n",
    "# Remove par da analise\n",
    "# Log de pares excluidos?\n",
    "# Sen√£o calcula diferen√ßa de cr√©ditos entre mat√©rias\n",
    "# Return lista de pares e tpei_dif deles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e34318",
   "metadata": {},
   "source": [
    "## 5. Filtro de Pr√©-requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DiGraph from prerequisite relationships\n",
    "# Generate node2vec embeddings for disciplines\n",
    "# Calculate cosine distance between discipline embeddings\n",
    "# Apply prerequisite similarity threshold\n",
    "# Filter pairs based on prerequisite similarity\n",
    "\n",
    "#  Constr√≥i grafo com arestas de pr√©-requisito com base na coluna RECOMENDACAO\n",
    "def construir_grafo(df: pd.DataFrame) -> nx.DiGraph:\n",
    "    print(\"üìå Construindo grafo de pr√©-requisitos...\")\n",
    "    G = nx.DiGraph()\n",
    "    total_arestas = 0\n",
    "\n",
    "    # Cria um dicion√°rio de nome normalizado ‚Üí sigla\n",
    "    mapping = {\n",
    "        normalizar_nome(row['DISCIPLINA']): row['SIGLA']\n",
    "        for _, row in df.iterrows()\n",
    "        if pd.notna(row['SIGLA'])\n",
    "    }\n",
    "\n",
    "    # Adiciona todos os n√≥s com metadados\n",
    "    for _, row in df.iterrows():\n",
    "        sigla = row['SIGLA']\n",
    "        nome = row['DISCIPLINA']\n",
    "        if pd.isna(sigla): \n",
    "            continue\n",
    "        G.add_node(sigla, nome=nome)\n",
    "\n",
    "    # Adiciona as arestas baseadas nas recomenda√ß√µes\n",
    "    for _, row in df.iterrows():\n",
    "        curso = row['SIGLA']\n",
    "        if pd.isna(curso):\n",
    "            continue\n",
    "\n",
    "        recs = row.get('RECOMENDACAO', '')\n",
    "        if pd.isna(recs) or not isinstance(recs, str):\n",
    "            continue\n",
    "\n",
    "        for rec in recs.split(';'):\n",
    "            rec = rec.strip()\n",
    "            if not rec:\n",
    "                continue\n",
    "            rec_norm = normalizar_nome(rec)\n",
    "            if rec_norm in mapping:\n",
    "                prereq = mapping[rec_norm]\n",
    "                if not G.has_edge(prereq, curso):\n",
    "                    G.add_edge(prereq, curso, tipo='pre_requisito')\n",
    "                    total_arestas += 1\n",
    "\n",
    "    print(f\"‚úÖ Grafo criado com {G.number_of_nodes()} n√≥s e {total_arestas} arestas.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc787716",
   "metadata": {},
   "source": [
    "## 6. C√°lculo de Score TPEI + Pr√©-requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara matriz de caracteristicas\n",
    "def prepare_combined_features(discipline_pairs, tpei_diff, prereq_similarities):\n",
    "    features = []\n",
    "    for pair in discipline_pairs:\n",
    "        prereq_sim = prereq_similarities.get(pairs, 0.0)\n",
    "        features.append([tpei_diff, prereq_sim])\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# Treina CatBoost usando labeled_data\n",
    "# C√≥digo da Larissa\n",
    "\n",
    "\n",
    "# Otimiza limiar\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "y_pred_proba = model_catboost.predict_proba(X_val)[:,1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Calcula score de cada par\n",
    "all_features = prepare_combined_features(candidate_pairs, tpei_diff, prereq_similarities)\n",
    "combined_scores = model_catboost.predict_proba(all_features)[:. 1]\n",
    "\n",
    "# Aplica filtro em cada par\n",
    "filtered_pairs = [\n",
    "    pair for pair, score in zip(candidate_pairs, combined_scores)\n",
    "    if score >= optimal_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5bf8a",
   "metadata": {},
   "source": [
    "# Sentence BERT\n",
    "\n",
    "O BERT √© um pacote que cria embeddings de palavras a partir de uma rede j√° treinada. Os embeddings carregam no seus valores o conteudo semantico de cada palavra a partir de uma representa√ß√£o vetorial de cada palavra\n",
    "\n",
    "O Sentence BERT ou SBERT utiliza esses embedings de cada palavra e calula o significado m√©dio, por exemplo, de cada frase, de forma que temos uma no√ß√£o do que cada senten√ßa significa.\n",
    "\n",
    "existem 1353 disciplinas e no primeiro momento calculamos a similaridade cosseno entre todos os pares. Assim a cossine_sim_bert ter√° a dimens√£o 1353x1353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3744b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sao_variantes_simples(nome1, nome2):\n",
    "    base1 = re.sub(r'[\\s\\-]*(laborat√≥rio|[a-zA-Z]{1,3}|[ivxIVX0-9]{1,4})\\s*$', '', nome1.strip(), flags=re.IGNORECASE)\n",
    "    base2 = re.sub(r'[\\s\\-]*(laborat√≥rio|[a-zA-Z]{1,3}|[ivxIVX0-9]{1,4})\\s*$', '', nome2.strip(), flags=re.IGNORECASE)\n",
    "    return base1.lower() == base2.lower()\n",
    "\n",
    "def similariry_between_DISCIPLINA(dataframe, cosine_sim, similarity_threshold = 0.75):\n",
    "    '''Encontrar pares com similaridade ‚â• 75% e faz o print dos nomes das disciplinas.\n",
    "        dataframe: DataFrame com os dados das disciplinas\n",
    "        cosine_sim: matriz de similaridade entre as disciplinas\n",
    "        similarity_threshold: limiar de similaridade (default: 0.75)'''\n",
    "\n",
    "    similar_pairs = []\n",
    "    n = len(dataframe)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):  # Evitar duplicatas (i, j) e (j, i)\n",
    "            nome_i = dataframe.iloc[i]['DISCIPLINA']\n",
    "            nome_j = dataframe.iloc[j]['DISCIPLINA']\n",
    "            if sao_variantes_simples(nome_i, nome_j):\n",
    "                continue  # pula se s√£o vers√µes quase id√™nticas da mesma disciplina\n",
    "            if cosine_sim[i, j] >= similarity_threshold:\n",
    "                similar_pairs.append((nome_i, nome_j, cosine_sim[i, j]))\n",
    "\n",
    "    # Exibir resultados\n",
    "    for pair in similar_pairs:\n",
    "        print(f\"Disciplinas similares: {pair[0]} e {pair[1]} (Similaridade: {pair[2]:.2f})\")\n",
    "    \n",
    "    return similar_pairs\n",
    "\n",
    "\n",
    "# Criar modelo SBERT para o portugu√™s para a coluna, usando o modelo BERT pr√©-treinado para portugu√™s\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Definir o modelo de pooling para agregar os embeddings. O pooling combina os embeddings de palavras em um √∫nico vetor\n",
    "# Usando o modo de pooling m√©dio, que calcula a m√©dia dos embeddings de palavras\n",
    "\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True)\n",
    "\n",
    "\n",
    "# Estancia o modelo SBERT com o modelo de embedding e o modelo de pooling\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "# Calcula embeddings para as EMENTAS\n",
    "embeddings = model.encode(\n",
    "    df['ementa_norm'].tolist(),\n",
    "    normalize_embeddings=True  # garante que todos os vetores t√™m norma 1\n",
    ")\n",
    "\n",
    "#calcular a similaridade entre os embeddings usando a similaridade do cosseno\n",
    "cosine_sim_sbert = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "# Fazer o print da similaridade entre as disciplinas com o threshold de 0.75\n",
    "sim_pair = similariry_between_DISCIPLINA(df, cosine_sim_sbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d5a30",
   "metadata": {},
   "source": [
    "## K-means \n",
    "\n",
    "vamos rodar o K-means para determinar se existe algum cluster de disciplinas que poderiamos agrupar. √â uma outra forma de entender similaridade agora visto com agrupamento de elemntos semelhantes para pensar as equivalencias internas. \n",
    "\n",
    "Come√ßamos por determinar os K otimo para esse m√©todo utilizando o metodo do cotovelo e o metodo da estatitica de gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964df801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazer um kmeans para encontrar o n√∫mero ideal de clusters utilizando o m√©todo do cotovelo\n",
    "# O m√©todo do cotovelo √© uma t√©cnica usada para determinar o n√∫mero ideal de clusters em um conjunto de dados\n",
    "\n",
    "def find_optimal_k(data, max_k=40):\n",
    "    # Lista para armazenar os valores de in√©rcia\n",
    "    inertia = []\n",
    "    \n",
    "    # Testar diferentes valores de k\n",
    "    for k in range(2, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plotar o gr√°fico do m√©todo do cotovelo\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(2, max_k + 1), inertia, marker='o')\n",
    "    plt.title('M√©todo do Cotovelo')\n",
    "    plt.xlabel('N√∫mero de Clusters (k)')\n",
    "    plt.ylabel('In√©rcia')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Chamar a fun√ß√£o para encontrar o n√∫mero ideal de clusters\n",
    "find_optimal_k(embeddings, max_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazer um kmeneans para encontrar o n√∫mero ideal de clusters utilizando o m√©todo da estatistica de Gap\n",
    "# O m√©todo da estat√≠stica de Gap √© uma t√©cnica usada para determinar o n√∫mero ideal de clusters em um conjunto de dados\n",
    "\n",
    "def gap_statistic(data, n_clusters_range, n_repeats=10):\n",
    "    # Lista para armazenar os valores de Gap\n",
    "    gaps = []\n",
    "    \n",
    "    # Calcular o Gap para cada valor de k\n",
    "    for k in n_clusters_range:\n",
    "        # Ajustar o modelo KMeans\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        \n",
    "        # Calcular a in√©rcia do modelo ajustado\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        # Gerar dados aleat√≥rios para compara√ß√£o\n",
    "        random_data = np.random.random_sample(size=data.shape)\n",
    "        \n",
    "        # Ajustar o modelo KMeans aos dados aleat√≥rios\n",
    "        kmeans_random = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans_random.fit(random_data)\n",
    "        \n",
    "        # Calcular a in√©rcia do modelo ajustado aos dados aleat√≥rios\n",
    "        inertia_random = kmeans_random.inertia_\n",
    "        \n",
    "        # Calcular o Gap e adicionar √† lista\n",
    "        gap = np.log(inertia_random) - np.log(inertia)\n",
    "        gaps.append(gap)\n",
    "    \n",
    "    return gaps\n",
    "\n",
    "# Definir o intervalo de k para testar\n",
    "n_clusters_range = range(30, 100)\n",
    "# Calcular a estat√≠stica de Gap\n",
    "gaps = gap_statistic(embeddings, n_clusters_range)\n",
    "# Plotar os resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_clusters_range, gaps, marker='o')\n",
    "plt.title('Estat√≠stica de Gap')\n",
    "plt.xlabel('N√∫mero de Clusters (k)')\n",
    "plt.ylabel('Gap')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d3830",
   "metadata": {},
   "source": [
    "Considerando que nos dois m√©todos que utilizamos de determina√ß√£o do numero otimo para o k deram inconclusivos vamos partir de uma hipotese a priori que toma em conta que na UFABC existem 31 cursos, sendo 4 BI/LI e 27 p√≥s-BI/LI. Alem disso para cada curso temos disciplinas obrigat√≥rias e limitadas. Assim vamos considerar a k=62 tendo em conta esses pontos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee47c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo o modelo de Kmeans para o numero de clusters determinado\n",
    "kmeans_final = KMeans(n_clusters=62, random_state=42, n_init=10)\n",
    "\n",
    "# Ajustar o modelo KMeans aos dados de embeddings\n",
    "# Encontrar disciplina que est√° no centroide de cada cluster\n",
    "kmeans_final.fit(embeddings)\n",
    "# Obter os r√≥tulos dos clusters\n",
    "labels = kmeans_final.labels_\n",
    "# Obter os centr√≥ides dos clusters\n",
    "centroids = kmeans_final.cluster_centers_\n",
    "# Calcular a dist√¢ncia euclidiana entre os embeddings e os centr√≥ides\n",
    "distances = euclidean_distances(embeddings, centroids)\n",
    "# Obter o √≠ndice do centr√≥ide mais pr√≥ximo para cada disciplina\n",
    "closest_centroid_indices = np.argmin(distances, axis=0)\n",
    "# printar os r√≥tulos dos clusters e os centr√≥ides\n",
    "for i in range(len(centroids)):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    # Obter os √≠ndices das disciplinas que pertencem ao cluster i\n",
    "    cluster_indices = np.where(labels == i)[0]\n",
    "    # Obter os nomes das disciplinas que pertencem ao cluster i\n",
    "    cluster_disciplines = df.iloc[cluster_indices]['DISCIPLINA'].tolist()\n",
    "    # Obter o nome da disciplina mais pr√≥xima do centr√≥ide\n",
    "    closest_discipline = df.iloc[closest_centroid_indices[i]]['DISCIPLINA']\n",
    "    print(f\"Disciplinas: {cluster_disciplines}\")\n",
    "    print(f\"Disciplina mais pr√≥xima do centr√≥ide: {closest_discipline}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f7907",
   "metadata": {},
   "source": [
    "## 8. C√°lculo de Score Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara caracteristicas\n",
    "def prepare_final_features(pairs, tpei_dif, prereq_sim, ementa_sim):\n",
    "    features = []\n",
    "    for pair in pairs:\n",
    "        prereq = prereq_sim.get(pair, 0.0)\n",
    "        ementa = ementa_sim(pair, 0.0)\n",
    "        features.append([tpei_dif, prereq, ementa])\n",
    "    return np.array(features)\n",
    "\n",
    "# Treino SVM com Kernel RBF\n",
    "# Codigo da Larissa\n",
    "\n",
    "# Calcula limiar √≥timo\n",
    "y_prob = svm_model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_final, y_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "final_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "all_final_features = prepare_final_features(\n",
    "    filtered_pairs, tpei_diff, prereq_sim, ementa_sim\n",
    ")\n",
    "all_scaled = scaler.transform(all_final_features)\n",
    "final_scores = svm_model.predict_proba(all_scaled)[:, 1]\n",
    "\n",
    "#Lista final de pares\n",
    "equivalent_pairs = [\n",
    "    (pair, score) for pair, score in zip(filtered_pairs, final_scores)\n",
    "    if score >= final_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff0ccc",
   "metadata": {},
   "source": [
    "## 9. Explica√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "explainer = shap.KernelExplainer(\n",
    "    svm_model.predict_proba,\n",
    "    X_scaled,\n",
    "    link=\"logit\"\n",
    ")\n",
    "\n",
    "shap_values = explainer.shap_values(all_scaled)\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "feature_names = ['Diferen√ßa de cr√©ditos', 'Similaridade de pr√©-requisito', 'Similaridade de ementa']\n",
    "importance = np.abs(shap_values).mean(0)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "def explain_prediction(pair_index, features, shap_values):\n",
    "    feature_values = features[pair_index]\n",
    "    shap_value = shap_values[pair_index]\n",
    "    \n",
    "    explanation = []\n",
    "    for i, (name, value, impact) in enumerate(zip(feature_names, feature_values, shap_value)):\n",
    "        direction = \"positive\" if impact > 0 else \"negative\"\n",
    "        explanation.append(f\"{name}: {value:.3f} ({direction} impact: {abs(impact):.3f})\")\n",
    "    \n",
    "    return \"\\n\".join(explanation)\n",
    "\n",
    "# Generate explanations for all equivalent pairs\n",
    "explanations = []\n",
    "for i, (pair, score) in enumerate(equivalent_pairs):\n",
    "    explanation = f\"Pair: {pair[0]} - {pair[1]}\\n\"\n",
    "    explanation += f\"Equivalence Score: {score:.3f}\\n\"\n",
    "    explanation += \"Feature Contributions:\\n\"\n",
    "    explanation += explain_prediction(i, all_final_features, shap_values)\n",
    "    explanations.append(explanation)\n",
    "# Initialize SHAP explainer for SVM model\n",
    "# Calculate SHAP values for each prediction\n",
    "# Generate feature importance rankings\n",
    "# Create individual prediction explanations\n",
    "# Prepare text explanations for results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79476c28",
   "metadata": {},
   "source": [
    "## 10. Visualiza√ß√µes dos Resultados\n",
    "\n",
    "### 10.1 Matriz de visualiza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar n√≥s (disciplinas)\n",
    "for sigla in df['DISCIPLINA'].unique():\n",
    "    G.add_node(sigla)\n",
    "\n",
    "# Modificar a cria√ß√£o de arestas\n",
    "for pair in similar_pairs:\n",
    "    disciplina_a, disciplina_b, similarity = pair\n",
    "    if similarity >= 0.8:  # Ajuste o threshold aqui\n",
    "        G.add_edge(disciplina_a, disciplina_b, weight=similarity)\n",
    "\n",
    "        # Calcular posi√ß√µes dos n√≥s\n",
    "        pos = nx.kamada_kawai_layout(G, weight='weight')  # Usa o peso (similaridade) para organizar\n",
    "\n",
    "        # Criar tra√ßos para arestas\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        for edge in G.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.extend([x0, x1, None])  # None para separar linhas\n",
    "            edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line= dict(width=1, color='#888'),  # Espessura e cor das arestas\n",
    "            hoverinfo='none',\n",
    "            mode='lines')\n",
    "\n",
    "# Criar tra√ßos para n√≥s\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_text = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)  # Texto ao passar o mouse\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "  x=node_x, y=node_y,\n",
    "  mode='markers+text',\n",
    "  text=node_text,\n",
    "  textposition=\"top center\",\n",
    "  hoverinfo='text',\n",
    "  marker=dict(\n",
    "      showscale=True,\n",
    "      colorscale='YlGnBu',\n",
    "      size=15,\n",
    "      color=[],  # Pode ser usado para codificar cores por comunidade\n",
    "      line=dict(width=2, color='black'))\n",
    ")\n",
    "\n",
    "# Criar figura\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "    layout=go.Layout(\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=0, l=0, r=0, t=0),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    ")\n",
    "\n",
    "# Adicionar interatividade (exibir sigla ao passar o mouse)\n",
    "fig.update_traces(textposition='top center', hoverinfo='text')\n",
    "\n",
    "from networkx.algorithms import community\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "# Atribuir cores diferentes a cada comunidade\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create DiGraph visualization showing prerequisite relationships\n",
    "# Color nodes based on equivalence status\n",
    "# Highlight connected discipline pairs\n",
    "# Add node labels and edge weights\n",
    "# Save graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80d967",
   "metadata": {},
   "source": [
    "### 10.2 Visualiza√ß√£o por mapa de calor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8245ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar matriz de similaridade como DataFrame\n",
    "similarity_df = pd.DataFrame(cosine_sim_bert, index=df['DISCIPLINA'], columns=df['DISCIPLINA'])\n",
    "\n",
    "# Plotar heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_df, cmap='YlGnBu', annot=False, mask=(similarity_df < 0.5))\n",
    "plt.title('Mapa de Calor de Similaridade entre Ementas')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Generate SHAP summary plot for feature importance\n",
    "# Create dependence plots for key features\n",
    "# Visualize force plots for individual predictions\n",
    "# Export plots for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcaaf6",
   "metadata": {},
   "source": [
    "### 10.3 Matrizes de Confus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbad8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix for model performance\n",
    "# Create heatmap visualization of confusion matrix\n",
    "# Add precision, recall, and F1 scores\n",
    "# Generate performance metrics report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868a7f5",
   "metadata": {},
   "source": [
    "## 11. Exporta√ß√£o de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f65c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered results to TSV file\n",
    "# Export model performance metrics\n",
    "# Save visualizations in specified formats\n",
    "# Generate final summary report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_disciplinas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
