{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343744a0",
   "metadata": {},
   "source": [
    "# Sistema de Detec√ß√£o de Equival√™ncia de Disciplinas\n",
    "\n",
    "# Notebook de verifica√ß√£o da similaridade entre ementas de disciplinas\n",
    "\n",
    "## Introdu√ß√£o:\n",
    "O estudante da UFABC passa muito tempo j√° em sua gradua√ß√£o para terminar as disciplinas do seu BI e do p√≥s-BI e isso afeta principalmente os alunos de cursos mais concorridos como √© o de computa√ß√£o, em que as disciplinas podem ter mais de 150% de requisi√ß√£o. Tendo em vista isso, na UFABC temos dois processos j√° estruturados que √© o processo de covalida√ß√£o e de equival√™ncia, normatizados nas resolu√ß√µes ConsEPE n¬∫ 157/2013 e CG N¬∫ 023/2019 respectivamente. Covalida√ß√£o √© um processo interno da UFABC que √© basicamente para a transi√ß√£o de projetos pedag√≥gicos, de forma que o(a) estudante consegue integralizar o curso em um PPC antigo com disciplinas novas e a equival√™ncia √© um processo que uma disciplina de fora pode ter alguma similaridade de uma disciplina da ufabc e do curso que voc√™ quer se formar. Assim est√° presente na Resolu√ß√£o CG N¬∫ 023/2019 o seguinte:\n",
    "\n",
    ">Art. 4¬∫ Consistem em requisitos para a dispensa por equival√™ncia, para disciplinas\n",
    "cursadas no Brasil:\n",
    "\n",
    ">> I. a carga hor√°ria total da disciplina cursada deve ser igual ou maior √† carga hor√°ria da que se pede equival√™ncia;\n",
    "\n",
    ">> II. o conte√∫do da disciplina cursada deve ser compat√≠vel e correspondente a, no m√≠nimo, 75% (setenta e cinco por cento) do conte√∫do daquela de que se pede equival√™ncia, considerando-se teoria e pr√°tica, quando for o caso. \n",
    "\n",
    ">>>Par√°grafo √∫nico: Excepcionalmente, e mediante justificativa, a coordena√ß√£o de curso pode autorizar equival√™ncias que cumpram parcialmente estes requisito\n",
    "\n",
    "Utilizando essa base das normativas e os cat√°logos de disciplinas da UFABC, objetivamos gerar um sistema de pr√©-avalia√ß√£o de disciplinas com alta chance de convalida√ß√£o, reduzindo o espa√ßo de busca dos t√©cnicos responsaveis pela aprova√ß√£o de pares de disciplinas com convalida√ß√£o e garantindo a inclus√£o de todas as disciplinas na an√°lise. Os benef√≠cio desta proposta s√£o a economia de recursos, maior integraliza√ß√£o de diferentes PPCs e promove efetivamente a interdisciplinaridade, fundamento da UFAB, uma vez que os cursos poderiam ofertar a mesma disciplina com diferentes enfoques no mesmo quadrimestre, melhorando a qualidade do ensino. Os recursos poupados s√£o tanto na carga de trabalho dos t√©cnicos quanto recursos computacionais (realizada manualmente, ess tarefa tem um complexidade O(n^2)), requerindo an√°lise manual apenas das disciplinas que se encaixam nos crit√©rios das resolu√ß√µes ConsEPE n¬∫ 157/2013 e CG N¬∫ 023/2019 e com alta probabilidade de valida√ß√£o. \n",
    "\n",
    "Dessa forma propomos uma an√°lise de similaridade semantica e relacional entre as ementas, os pr√©-requisitos das disciplinas da UFABC e dos valores de TPEI, para fim de pr√©-avalia√ß√£o de equival√™ncias entre as diciplinas da universidade cumprindo com o Art. 4¬∞ da resolu√ß√£o CG N¬∫ 023/2019. Assim. em disciplinas que verificarmos que existe uma similaridade equivalente a de pares de disciplinas atualmente validadas (similaridade maior ou igual a 75% e que cumpre a quantidade de creditos da outra disciplina), poderemos gerar listas de pares para aprova√ß√£o manual em ordem de similaridde. O benef√≠cio proporcionado pela maior integra√ß√£o do sistema √© dif√≠cil de mensurar mas, al√©m de afetar todes es envolvides no processo discente, o alinhamento dessa solu√ß√£o com os sistemas vigentes da universidade amplificam seu impacto e condi√ß√µes em que ele se manifesta.\n",
    "\n",
    "## Objetivos\n",
    "1. Fazer uma proposta de equival√™ncia interna de disciplinas a partir da an√°lise de similaridade sem√¢ntica entre as ementas das disciplinas da UFABC da Gradua√ß√£o e da quantidade de TPEI que elas tem\n",
    "2. Reduzir o espa√ßo de busca de O(n¬≤) para aproximadamente O(n*log(n)) a partir da aplica√ß√£o de filtros de TPEI para quantidade de cr√©ditos das disciplinas, de forma s√≥ comparar as diciplinas que tem a quantidade de cr√©ditos iguais\n",
    "\n",
    "## M√©todos\n",
    "1. Embedding sema√¢ntico do conte√∫do dos Objetivos e Ementas das disciplinas utilizando BERTimbau e o c√°lculo de similaridades de cosseno\n",
    "2. Grafo relacional dos Pr√©-Requisitos entre disciplinas e c√°lculo de simil√ßaridade utiliando dist√¢ncias de jaccard\n",
    "3. Otimiza√ß√£o dos hiperpar√¢metros dos filtros de redu√ß√£o do espa√ßo de busca utilizando modelos de √°rvores aleat√≥rias \n",
    "\n",
    "## 1. Imports e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3772ff9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'node2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnode2vec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Node2Vec\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'node2vec'"
     ]
    }
   ],
   "source": [
    "# Realiza imports necess√°rios\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import unicodedata\n",
    "import requests\n",
    "from io import StringIO\n",
    "from node2vec import Node2Vec\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85da40",
   "metadata": {},
   "source": [
    "## 2. Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixa o cat√°logo da UFABC direto do Reposit√≥rio no GitHub\n",
    "def carregar_catalogo():\n",
    "    url = \"https://raw.githubusercontent.com/angeloodr/disciplinas-ufabc/main/catalogo_disciplinas_graduacao_2024_2025.tsv\"\n",
    "    print(\"üîÑ Baixando cat√°logo de disciplinas...\")\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    df = pd.read_csv(StringIO(resp.text), sep='\\t')\n",
    "\n",
    "    # Normaliza os nomes das colunas\n",
    "    df.columns = [\n",
    "        normalize_str(col).upper().replace(' ', '_')\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    print(\"‚úÖ Download bem-sucedido!\")\n",
    "    print(\"üìù Colunas dispon√≠veis:\", df.columns.tolist())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7a644",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai dados de TPEI\n",
    "#def parse_tpei(tpei_str):\n",
    "#    # Converts \"2-0-0-2\" to dict with numeric values\n",
    "#    values = tpei_str.split('-')\n",
    "#    return {'T': int(values[0]), 'P': int(values[1]), \n",
    "#            'E': int(values[2]), 'I': int(values[3])}\n",
    "#\n",
    "# catalog_df['TPEI_parsed'] = catalog_df['TPEI'].apply(parse_tpei)\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    norm = unicodedata.normalize('NFKD', str(text))\n",
    "    norm = norm.encode('ASCII', 'ignore').decode('utf-8')\n",
    "    norm = re.sub(r'[^\\w\\s]', '')\n",
    "    norm = norm.lower().strip()\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def extract_tpei(tpei_str):\n",
    "    if pd.isna(tpei_str):\n",
    "        return []\n",
    "    \n",
    "    values = tpei_str.split('-')\n",
    "    return {\n",
    "        'teoria': int(values[0]),\n",
    "        'pratica': int(values[1]),\n",
    "        'extensao': int(values[2]),\n",
    "        'individual': int(values[3]),\n",
    "        'total_creditos': int(values[0]) + int(values[1])  # T+P only\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def extract_prereq(recomendacao):\n",
    "    if pd.isna(recomendacao) or recomendacao.strip() == '':\n",
    "        return []\n",
    "\n",
    "    prereqs = []\n",
    "    for part in recomendacao.split(';'):\n",
    "        part = part.strip()\n",
    "        if part:\n",
    "            prereqs.append(part)\n",
    "    return prereqs\n",
    "\n",
    "def extract_cod(sigla):\n",
    "    if pd.isna(sigla):\n",
    "        return []\n",
    "    return normalize_text(sigla)\n",
    "\n",
    "def create_allfeats(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # TPEI\n",
    "    tpei_feats = df['TPEI'].apply(extract_tpei)\n",
    "    df['teoria'] = tpei_feats.apply(lambda x: x['teoria'])\n",
    "    df['pratica'] = tpei_feats.apply(lambda x: x['pratica'])\n",
    "    df['extensao'] = tpei_feats.apply(lambda x: x['extensao'])\n",
    "    df['individual'] = tpei_feats.apply(lambda x: x['individual'])\n",
    "    df['total_creditos'] = tpei_feats.apply(lambda x: x['total_creditos'])\n",
    "\n",
    "    # Ementa\n",
    "    df['ementa_norm'] = df['EMENTA'].apply(normalize_text)\n",
    "    df['objetivos_norm'] = df['OBJETIVOS'].apply(normalize_text)\n",
    "\n",
    "    # Pr√©-requisitos\n",
    "    df['prerequisites'] = df['RECOMENDA√á√ÉO'].apply(extract_prereq)\n",
    "    df['num_prerequisites'] = df['prerequisites'].apply(len)\n",
    "    df['codigo'] = df['SIGLA'].apply(extract_cod)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd40eb",
   "metadata": {},
   "source": [
    "## 4. Filtro TPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pair in pairs\n",
    "#   se total creditos[current] > total_creditos[comp]\n",
    "#       remove.pair\n",
    "#   else\n",
    "#       tpei_dif = comp - current\n",
    "#\n",
    "# Se os cr√©ditos totais da disciplina sendo comparada for maior do que da disciplina sendo analisada\n",
    "# Remove par da analise\n",
    "# Log de pares excluidos?\n",
    "# Sen√£o calcula diferen√ßa de cr√©ditos entre mat√©rias\n",
    "# Return lista de pares e tpei_dif deles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e34318",
   "metadata": {},
   "source": [
    "## 5. Filtro de Pr√©-requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DiGraph from prerequisite relationships\n",
    "# Generate node2vec embeddings for disciplines\n",
    "# Calculate cosine distance between discipline embeddings\n",
    "# Apply prerequisite similarity threshold\n",
    "# Filter pairs based on prerequisite similarity\n",
    "\n",
    "#  Constr√≥i grafo com arestas de pr√©-requisito com base na coluna RECOMENDACAO\n",
    "def construir_grafo(df: pd.DataFrame) -> nx.DiGraph:\n",
    "    print(\"üìå Construindo grafo de pr√©-requisitos...\")\n",
    "    G = nx.DiGraph()\n",
    "    total_arestas = 0\n",
    "\n",
    "    # Cria um dicion√°rio de nome normalizado ‚Üí sigla\n",
    "    mapping = {\n",
    "        normalizar_nome(row['DISCIPLINA']): row['SIGLA']\n",
    "        for _, row in df.iterrows()\n",
    "        if pd.notna(row['SIGLA'])\n",
    "    }\n",
    "\n",
    "    # Adiciona todos os n√≥s com metadados\n",
    "    for _, row in df.iterrows():\n",
    "        sigla = row['SIGLA']\n",
    "        nome = row['DISCIPLINA']\n",
    "        if pd.isna(sigla): \n",
    "            continue\n",
    "        G.add_node(sigla, nome=nome)\n",
    "\n",
    "    # Adiciona as arestas baseadas nas recomenda√ß√µes\n",
    "    for _, row in df.iterrows():\n",
    "        curso = row['SIGLA']\n",
    "        if pd.isna(curso):\n",
    "            continue\n",
    "\n",
    "        recs = row.get('RECOMENDACAO', '')\n",
    "        if pd.isna(recs) or not isinstance(recs, str):\n",
    "            continue\n",
    "\n",
    "        for rec in recs.split(';'):\n",
    "            rec = rec.strip()\n",
    "            if not rec:\n",
    "                continue\n",
    "            rec_norm = normalizar_nome(rec)\n",
    "            if rec_norm in mapping:\n",
    "                prereq = mapping[rec_norm]\n",
    "                if not G.has_edge(prereq, curso):\n",
    "                    G.add_edge(prereq, curso, tipo='pre_requisito')\n",
    "                    total_arestas += 1\n",
    "\n",
    "    print(f\"‚úÖ Grafo criado com {G.number_of_nodes()} n√≥s e {total_arestas} arestas.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc787716",
   "metadata": {},
   "source": [
    "## 6. C√°lculo de Score TPEI + Pr√©-requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara matriz de caracteristicas\n",
    "def prepare_combined_features(discipline_pairs, tpei_diff, prereq_similarities):\n",
    "    features = []\n",
    "    for pair in discipline_pairs:\n",
    "        prereq_sim = prereq_similarities.get(pairs, 0.0)\n",
    "        features.append([tpei_diff, prereq_sim])\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# Treina CatBoost usando labeled_data\n",
    "# C√≥digo da Larissa\n",
    "\n",
    "\n",
    "\n",
    "# Otimiza limiar\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "y_pred_proba = model_catboost.predict_proba(X_val)[:,1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Calcula score de cada par\n",
    "all_features = prepare_combined_features(candidate_pairs, tpei_diff, prereq_similarities)\n",
    "combined_scores = model_catboost.predict_proba(all_features)[:. 1]\n",
    "\n",
    "# Aplica filtro em cada par\n",
    "filtered_pairs = [\n",
    "    pair for pair, score in zip(candidate_pairs, combined_scores)\n",
    "    if score >= optimal_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5bf8a",
   "metadata": {},
   "source": [
    "## 7. Filtro de Ementa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3744b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERTimbau pre-trained model\n",
    "# Generate embeddings for ementa texts\n",
    "# Apply node2vec to ementa embeddings\n",
    "# Calculate cosine distance between ementa embeddings\n",
    "# Filter based on semantic similarity threshold\n",
    "\n",
    "import re\n",
    "\n",
    "def sao_variantes_simples(nome1, nome2):\n",
    "    base1 = re.sub(r'[\\s\\-]*(laborat√≥rio|[a-zA-Z]{1,3}|[ivxIVX0-9]{1,4})\\s*$', '', nome1.strip(), flags=re.IGNORECASE)\n",
    "    base2 = re.sub(r'[\\s\\-]*(laborat√≥rio|[a-zA-Z]{1,3}|[ivxIVX0-9]{1,4})\\s*$', '', nome2.strip(), flags=re.IGNORECASE)\n",
    "    return base1.lower() == base2.lower()\n",
    "\n",
    "def similariry_between_DISCIPLINA(cosine_sim, similarity_threshold = 0.75):\n",
    "    # Encontrar pares com similaridade ‚â• 75%\n",
    "    similar_pairs = []\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):  # Evitar duplicatas (i, j) e (j, i)\n",
    "            nome_i = df.iloc[i]['DISCIPLINA']\n",
    "            nome_j = df.iloc[j]['DISCIPLINA']\n",
    "            if sao_variantes_simples(nome_i, nome_j):\n",
    "                continue  # pula se s√£o vers√µes quase id√™nticas da mesma disciplina\n",
    "            if cosine_sim[i, j] >= similarity_threshold:\n",
    "                similar_pairs.append((nome_i, nome_j, cosine_sim[i, j]))\n",
    "\n",
    "    # Exibir resultados\n",
    "    for pair in similar_pairs:\n",
    "        print(f\"Disciplinas similares: {pair[0]} e {pair[1]} (Similaridade: {pair[2]:.2f})\")\n",
    "    \n",
    "    return similar_pairs\n",
    "\n",
    "model = SentenceTransformer('neuralmind/bert-base-portuguese-cased')\n",
    "embeddings = model.encode(df['EMENTA_PREPROCESSED'].tolist())\n",
    "cosine_sim_bert = cosine_similarity(embeddings, embeddings)\n",
    "sim_pair = similariry_between_DISCIPLINA(cosine_sim_bert)\n",
    "\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "# Criar modelo SBERT a partir do BERT\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode_mean_tokens=True)\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "embeddings = model.encode(df['EMENTA_PREPROCESSED'].tolist())\n",
    "cosine_sim_sbert = cosine_similarity(embeddings, embeddings)\n",
    "sim_pair = similariry_between_DISCIPLINA(cosine_sim_sbert)\n"
]

  },
  {
   "cell_type": "markdown",
   "id": "df0f7907",
   "metadata": {},
   "source": [
    "## 8. C√°lculo de Score Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara caracteristicas\n",
    "def prepare_final_features(pairs, tpei_dif, prereq_sim, ementa_sim):\n",
    "    features = []\n",
    "    for pair in pairs:\n",
    "        prereq = prereq_sim.get(pair, 0.0)\n",
    "        ementa = ementa_sim(pair, 0.0)\n",
    "        features.append([tpei_dif, prereq, ementa])\n",
    "    return np.array(features)\n",
    "\n",
    "# Treino SVM com Kernel RBF\n",
    "# Codigo da Larissa\n",
    "\n",
    "# Calcula limiar √≥timo\n",
    "y_prob = svm_model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_final, y_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "final_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "all_final_features = prepare_final_features(\n",
    "    filtered_pairs, tpei_diff, prereq_sim, ementa_sim\n",
    ")\n",
    "all_scaled = scaler.transform(all_final_features)\n",
    "final_scores = svm_model.predict_proba(all_scaled)[:, 1]\n",
    "\n",
    "#Lista final de pares\n",
    "equivalent_pairs = [\n",
    "    (pair, score) for pair, score in zip(filtered_pairs, final_scores)\n",
    "    if score >= final_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff0ccc",
   "metadata": {},
   "source": [
    "## 9. Explica√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "explainer = shap.KernelExplainer(\n",
    "    svm_model.predict_proba,\n",
    "    X_scaled,\n",
    "    link=\"logit\"\n",
    ")\n",
    "\n",
    "shap_values = explainer.shap_values(all_scaled)\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "feature_names = ['Diferen√ßa de cr√©ditos', 'Similaridade de pr√©-requisito', 'Similaridade de ementa']\n",
    "importance = np.abs(shap_values).mean(0)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "def explain_prediction(pair_index, features, shap_values):\n",
    "    feature_values = features[pair_index]\n",
    "    shap_value = shap_values[pair_index]\n",
    "    \n",
    "    explanation = []\n",
    "    for i, (name, value, impact) in enumerate(zip(feature_names, feature_values, shap_value)):\n",
    "        direction = \"positive\" if impact > 0 else \"negative\"\n",
    "        explanation.append(f\"{name}: {value:.3f} ({direction} impact: {abs(impact):.3f})\")\n",
    "    \n",
    "    return \"\\n\".join(explanation)\n",
    "\n",
    "# Generate explanations for all equivalent pairs\n",
    "explanations = []\n",
    "for i, (pair, score) in enumerate(equivalent_pairs):\n",
    "    explanation = f\"Pair: {pair[0]} - {pair[1]}\\n\"\n",
    "    explanation += f\"Equivalence Score: {score:.3f}\\n\"\n",
    "    explanation += \"Feature Contributions:\\n\"\n",
    "    explanation += explain_prediction(i, all_final_features, shap_values)\n",
    "    explanations.append(explanation)\n",
    "# Initialize SHAP explainer for SVM model\n",
    "# Calculate SHAP values for each prediction\n",
    "# Generate feature importance rankings\n",
    "# Create individual prediction explanations\n",
    "# Prepare text explanations for results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79476c28",
   "metadata": {},
   "source": [
    "## 10. Visualiza√ß√µes dos Resultados\n",
    "\n",
    "### 10.1 Matriz de visualiza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar n√≥s (disciplinas)\n",
    "for sigla in df['DISCIPLINA'].unique():\n",
    "    G.add_node(sigla)\n",
    "\n",
    "# Modificar a cria√ß√£o de arestas\n",
    "for pair in similar_pairs:\n",
    "    disciplina_a, disciplina_b, similarity = pair\n",
    "    if similarity >= 0.8:  # Ajuste o threshold aqui\n",
    "        G.add_edge(disciplina_a, disciplina_b, weight=similarity)\n",
    "\n",
    "        # Calcular posi√ß√µes dos n√≥s\n",
    "        pos = nx.kamada_kawai_layout(G, weight='weight')  # Usa o peso (similaridade) para organizar\n",
    "\n",
    "        # Criar tra√ßos para arestas\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        for edge in G.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.extend([x0, x1, None])  # None para separar linhas\n",
    "            edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line= dict(width=1, color='#888'),  # Espessura e cor das arestas\n",
    "            hoverinfo='none',\n",
    "            mode='lines')\n",
    "\n",
    "# Criar tra√ßos para n√≥s\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_text = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)  # Texto ao passar o mouse\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "  x=node_x, y=node_y,\n",
    "  mode='markers+text',\n",
    "  text=node_text,\n",
    "  textposition=\"top center\",\n",
    "  hoverinfo='text',\n",
    "  marker=dict(\n",
    "      showscale=True,\n",
    "      colorscale='YlGnBu',\n",
    "      size=15,\n",
    "      color=[],  # Pode ser usado para codificar cores por comunidade\n",
    "      line=dict(width=2, color='black'))\n",
    ")\n",
    "\n",
    "# Criar figura\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "    layout=go.Layout(\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=0, l=0, r=0, t=0),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    ")\n",
    "\n",
    "# Adicionar interatividade (exibir sigla ao passar o mouse)\n",
    "fig.update_traces(textposition='top center', hoverinfo='text')\n",
    "\n",
    "from networkx.algorithms import community\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "# Atribuir cores diferentes a cada comunidade\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create DiGraph visualization showing prerequisite relationships\n",
    "# Color nodes based on equivalence status\n",
    "# Highlight connected discipline pairs\n",
    "# Add node labels and edge weights\n",
    "# Save graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80d967",
   "metadata": {},
   "source": [
    "### 10.2 Visualiza√ß√£o por mapa de calor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8245ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar matriz de similaridade como DataFrame\n",
    "similarity_df = pd.DataFrame(cosine_sim_bert, index=df['DISCIPLINA'], columns=df['DISCIPLINA'])\n",
    "\n",
    "# Plotar heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_df, cmap='YlGnBu', annot=False, mask=(similarity_df < 0.5))\n",
    "plt.title('Mapa de Calor de Similaridade entre Ementas')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Generate SHAP summary plot for feature importance\n",
    "# Create dependence plots for key features\n",
    "# Visualize force plots for individual predictions\n",
    "# Export plots for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcaaf6",
   "metadata": {},
   "source": [
    "### 10.3 Matrizes de Confus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbad8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix for model performance\n",
    "# Create heatmap visualization of confusion matrix\n",
    "# Add precision, recall, and F1 scores\n",
    "# Generate performance metrics report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868a7f5",
   "metadata": {},
   "source": [
    "## 11. Exporta√ß√£o de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f65c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered results to TSV file\n",
    "# Export model performance metrics\n",
    "# Save visualizations in specified formats\n",
    "# Generate final summary report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
